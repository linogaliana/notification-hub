{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "This is a frame of the `main` notebook. It contains the training of Google's **T5** model.  "
      ],
      "metadata": {
        "id": "nCvQpdaxEKe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III.1. Loading the model\n",
        "\n",
        "Now that we have completely preprocessed the dataset. We are going to start working around the final model we used.\n",
        "\n",
        "Because of the computionnal power we have, we will have to use a relatively light model well suited for text summarization.\n",
        "\n",
        "Here, we will be working with `Google-T5` model. First let's load the model.  "
      ],
      "metadata": {
        "id": "FVg0WFVJiTYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "max_token_limit = tokenizer.model_max_length\n",
        "print(\"max_token_limit\", max_token_limit)\n",
        "\n",
        "max_input_length = max_token_limit\n",
        "max_target_length = 30\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "ouhU2iamp3Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III.2. Tokenization\n",
        "\n",
        "As such, we can not use directly the dataset because AI models are not really suited to handle texts. However they can manage numbers therefore the first step is to tokenize text.\n",
        "\n",
        "Also because we are using Hugging Face models, the **reference** summaries are called **labels**  "
      ],
      "metadata": {
        "id": "YlYssNwYjX2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEsoetCYLtFv"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(examples):\n",
        "    model_inputs = tokenizer(examples['dialogue'], max_length=max_input_length, truncation=True)\n",
        "    labels = tokenizer(examples['generated_summary'], max_length=max_target_length, truncation=True)\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = ds.map(tokenize_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with a small dataset is very useful for debugging. This line of command will allow us to get a sample of the full dataset. However when training the final version of the model, we have been using the full dataset."
      ],
      "metadata": {
        "id": "9RwtcmrPkUvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset = tokenized_dataset.filter(lambda e, i: i < 10, with_indices=True)\n",
        "small_dataset"
      ],
      "metadata": {
        "id": "-4Bp9tm_hozc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III.3. Training and evaluation of the model"
      ],
      "metadata": {
        "id": "33ZCBoXugZ8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. Evaluation of the model's quality"
      ],
      "metadata": {
        "id": "3COub5bpijRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First in order to train our model, we have to compute a metric to tell the model how well it performs. Again, we will be using the ROUGE score metric in order to measure the quality of the summary."
      ],
      "metadata": {
        "id": "-uX5INXUlrmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V38uX27ANEiJ"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_predictions = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_predictions]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = [scorer.score(predictions, labels) for predictions, labels in zip(decoded_predictions, decoded_labels)]\n",
        "\n",
        "    rougeL = [score['rougeL'].fmeasure * 100 for score in result]\n",
        "    rouge1 = [score['rouge1'].fmeasure * 100 for score in result]\n",
        "\n",
        "    result = {\n",
        "        'rougeL': sum(rougeL)/len(rougeL),\n",
        "        'rouge1': sum(rouge1)/len(rouge1),\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Training and quality of the final model"
      ],
      "metadata": {
        "id": "RfHn_gdKizOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weâ€™ll need to generate summaries in order to compute ROUGE scores during training.\n",
        "\n",
        "The `Seq2SeqTrainingArguments` class will be used to do this work."
      ],
      "metadata": {
        "id": "NvTVIyQ5mHSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9CBi2TmPYce"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"notification-hub\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `DataCollatorForSeq2Seq` collator will dynamically pad the inputs and the labels. It is required because our model  is an encoder-decoder Transformer model, which means that during decoding we need to shift the labels to the right by one."
      ],
      "metadata": {
        "id": "HHgiiCwum9V_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhghHhlkQ94Q"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we need to instantiate the trainer with the standard arguments.\n"
      ],
      "metadata": {
        "id": "rczzRdX8nrqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LcdewiZRBEr"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our model."
      ],
      "metadata": {
        "id": "wmRmyJaDn25i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "v-kUN6RgRmIt",
        "outputId": "65dc3313-c300-49f7-af24-f759ba307626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9928' max='9928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9928/9928 1:12:28, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rouge1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.635800</td>\n",
              "      <td>1.555176</td>\n",
              "      <td>37.269752</td>\n",
              "      <td>43.354466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.475800</td>\n",
              "      <td>1.543690</td>\n",
              "      <td>38.159236</td>\n",
              "      <td>43.892302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.399100</td>\n",
              "      <td>1.553527</td>\n",
              "      <td>38.264955</td>\n",
              "      <td>44.083064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.336500</td>\n",
              "      <td>1.562817</td>\n",
              "      <td>38.419383</td>\n",
              "      <td>44.561430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9928, training_loss=1.4745837387589078, metrics={'train_runtime': 4348.3588, 'train_samples_per_second': 9.133, 'train_steps_per_second': 2.283, 'total_flos': 1.457011180505088e+16, 'train_loss': 1.4745837387589078, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see the final ROUGE score"
      ],
      "metadata": {
        "id": "kI2OI6_Sn8mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "5dY4MnrXo9ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be pushing our model to Hugging Face in order to use it."
      ],
      "metadata": {
        "id": "gqxzEV9doGFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
      ],
      "metadata": {
        "id": "xS_b-RkKpSVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}