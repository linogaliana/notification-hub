{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup\n",
        "\n",
        "This is a frame of the `main` notebook. It contains code to set up the environment and load the LLama pipeline."
      ],
      "metadata": {
        "id": "pELB1Jw7OqbB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCjW20GuBz-O"
      },
      "source": [
        "# Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models and datasets.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU.\n",
        "- `rouge_score`: Allows us to get the ROUGE metric on Python.\n",
        "- `datasets`: Allows us to load datasets from the transformer library.\n",
        "- `py7zr`: Required in order to use the SAMSum corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktn8OG5rPgcl"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score transformers torch accelerate datasets py7zr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzONthxzCqXz"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "To load our desired model, `meta-llama/Llama-2-7b-chat-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVchIqEQCxev"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocmtwn8yC70C"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#I. Creation of the work environment"
      ],
      "metadata": {
        "id": "pjWyorjJG6pd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OEEaESgsAlI"
      },
      "source": [
        "# I.1. Loading Dataset\n",
        "\n",
        "We are going to analyze the dataset and see how we could use it to best fit our purpose.\n",
        "\n",
        "First, let's load the dataset and see how it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfQkbJbzsIYH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"samsum\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3HoqZDaFJZW"
      },
      "source": [
        "Now we know that we have three subdatasets: `train`, `test`, `validation`.\n",
        "\n",
        "It will come in handy to train and evaluate the fine-tuned model but let's look a little bit further and see how a row looks like.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tgBifOqC8DJ",
        "outputId": "0e5b3dab-3bd3-4f06-c5ea-e00972a25894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row: {'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'} \n",
            "\n",
            "Amanda: I baked  cookies. Do you want some?\r\n",
            "Jerry: Sure!\r\n",
            "Amanda: I'll bring you tomorrow :-) \n",
            "\n",
            ">>  Amanda baked cookies and will bring Jerry some tomorrow. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def show_row(dataset):\n",
        "  \"\"\"\n",
        "  Show the first row of the dataset.\n",
        "\n",
        "  Parameters:\n",
        "        dataset (Dataset): The dataset which will be used to train the AI.\n",
        "\n",
        "  Returns:\n",
        "      None: Prints the row.\n",
        "  \"\"\"\n",
        "  sample = dataset['train'].select(range(1))\n",
        "  for line in sample:\n",
        "    print('Row:',  line, '\\n')\n",
        "    print(line['dialogue'], '\\n')\n",
        "    print('>> ', line['summary'], '\\n')\n",
        "\n",
        "show_row(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MvGkwqG_QC"
      },
      "source": [
        "The summary is a third-person utterance. However we want to train our model to summarize the message in the first-person to make it more conversational.\n",
        "\n",
        "Also the corpus contains around 25% of dialogues with more than two characters in it. We want to get rid of them because it will be harder to transform it to a first-person perspective with an AI model.\n",
        "\n",
        "We will be using Meta's model `LLama2` as it is a top-notch open source model handling text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3IKEkUpDLjN"
      },
      "source": [
        "# I.2. Loading Model & Tokenizer\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBsFUVqHDNsg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ne-y66FDZOK"
      },
      "source": [
        "# I.3. Creating the Llama Pipeline\n",
        "\n",
        "We'll set up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n",
        "\n",
        "*Note*: This cell takes 2-3 minutes to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ23SHtxDTQw"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import accelerate\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JAvaUF4MVts"
      },
      "outputs": [],
      "source": [
        "def get_llama_response(prompts):\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompts,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "    return [sequence[0]['generated_text'] for sequence in sequences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KylHREcUuQI"
      },
      "source": [
        "We will be working with the following prompt:\n",
        "\n",
        "*Transform the following third-person sentence into first-person. Replace `Name1`'s pronouns and possessive determiners with first-person counterparts. Replace `Name2`'s pronouns and possessive determiners with second-person counterparts.  Adjust verb forms accordingly. Write the transformed sentence only and write this mention at the begining: 'Answer:'. '`Summary`'*\n",
        "\n",
        "Let's try it out with the example above.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geMiektTWa8Y"
      },
      "outputs": [],
      "source": [
        "prompt = \"Transform the following third-person sentence into first-person. Replace Amanda's pronouns and possessive determiners with first-person counterparts. Replace Jerry's pronouns and possessive determiners with second-person counterparts. Adjust verb forms accordingly. Write the transformed sentence only and write this mention at the begining: 'Answer:'. 'Amanda baked cookies and will bring Jerry some tomorrow. '\"\n",
        "responses = get_llama_response([prompt])\n",
        "\n",
        "print(responses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOOR-ADDYzCF"
      },
      "source": [
        "You might not get a satisfying response on your first attempt but if you retry a few attemps you should get something like this:\n",
        "\n",
        "*Answer: I baked cookies and will bring you some tomorrow.*\n",
        "\n",
        "Here the LLama's response is satisfying. However we are going to introduce a metric later in order to evaluate the performance of the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}